{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK7ecnb6pKzp"
   },
   "source": [
    "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6cR-I1WpKzs"
   },
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 14._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPbJEmZpKzu"
   },
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:13:52.909342: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-19 16:13:52.909376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-19 16:13:52.909404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-19 16:13:52.915721: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 16:13:53.533238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version:  1.4.2\n",
      "TF version:  2.14.0\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:13:54.659024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:54.688876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:54.689214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:54.693844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:54.694147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:54.694411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:55.216679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:55.216987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:55.217259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 16:13:55.217481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46681 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:c1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from packaging import version\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
    "\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oid44Xx-pKz6"
   },
   "source": [
    "# CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZe7PLfpKz6"
   },
   "source": [
    "**Tackling Fashion MNIST With a CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads the mnist dataset, add the channels axis to the inputs,\n",
    "#              scales the values to the 0-1 range, and splits the dataset\n",
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'Model': [], 'accuracy': [], 'training_time': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        3200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1413834 (5.39 MB)\n",
      "Trainable params: 1413834 (5.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:13:58.253963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8800\n",
      "2024-05-19 16:14:00.417170: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4b89e28590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-19 16:14:00.417203: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-05-19 16:14:00.424708: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-19 16:14:00.524330: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 16s 6ms/step - loss: 0.7446 - accuracy: 0.7373 - val_loss: 0.3633 - val_accuracy: 0.8712\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4241 - accuracy: 0.8571 - val_loss: 0.3239 - val_accuracy: 0.8826\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3603 - accuracy: 0.8794 - val_loss: 0.3060 - val_accuracy: 0.8922\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3165 - accuracy: 0.8934 - val_loss: 0.2707 - val_accuracy: 0.9002\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2904 - accuracy: 0.9030 - val_loss: 0.2840 - val_accuracy: 0.8926\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3130 - accuracy: 0.8901\n",
      "1/1 [==============================] - 0s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "# extra code – compiles, fits, evaluates, and uses the model to make predictions\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "n_epoch = 5\n",
    "# With GPU, train for 10 epochs\n",
    "# n_epoch=10\n",
    "history = model.fit(X_train, y_train, epochs=n_epoch,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "# Evaluation\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10]  # pretend we have new images\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL6mFjsUaoHT"
   },
   "source": [
    "# Exercise 14.1\n",
    "- Construct simplified LeNet-5 as shown in the table\n",
    "- ReLu activation\n",
    "- Ignore S2->C3 connection and consider regular connection\n",
    "- Dropout rate: 0.5 for FC\n",
    "- Output layer: softmax\n",
    "- Train and evalute the LeNet-5 model and compare the results of the model in the practice code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 64)        3200      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1413834 (5.39 MB)\n",
      "Trainable params: 1413834 (5.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 4s 15ms/step - loss: 1.4179 - accuracy: 0.4809 - val_loss: 0.5673 - val_accuracy: 0.7962\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.7913 - accuracy: 0.7060 - val_loss: 0.4329 - val_accuracy: 0.8544\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.6258 - accuracy: 0.7719 - val_loss: 0.3916 - val_accuracy: 0.8702\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.5356 - accuracy: 0.8078 - val_loss: 0.3413 - val_accuracy: 0.8806\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.4666 - accuracy: 0.8347 - val_loss: 0.3264 - val_accuracy: 0.8884\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.4317 - accuracy: 0.8499 - val_loss: 0.3106 - val_accuracy: 0.8940\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.3925 - accuracy: 0.8623 - val_loss: 0.2866 - val_accuracy: 0.8984\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.3643 - accuracy: 0.8746 - val_loss: 0.2599 - val_accuracy: 0.9062\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.3401 - accuracy: 0.8825 - val_loss: 0.2671 - val_accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.3153 - accuracy: 0.8933 - val_loss: 0.2500 - val_accuracy: 0.9088\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2704 - accuracy: 0.9073\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=n_epoch,\n",
    "                    validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Model in practice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QK1AM86ZaoHT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 120)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                10164     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define simplified LeNet-5\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=120, kernel_size=(5, 5), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_x17eKUjaoHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 2s 4ms/step - loss: 1.1077 - accuracy: 0.6033 - val_loss: 0.6166 - val_accuracy: 0.7646\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.6563 - accuracy: 0.7565 - val_loss: 0.5110 - val_accuracy: 0.8096\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5737 - accuracy: 0.7894 - val_loss: 0.4752 - val_accuracy: 0.8216\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.5231 - accuracy: 0.8105 - val_loss: 0.4443 - val_accuracy: 0.8356\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4954 - accuracy: 0.8223 - val_loss: 0.4196 - val_accuracy: 0.8430\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.4704 - accuracy: 0.8322 - val_loss: 0.4102 - val_accuracy: 0.8478\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.8389 - val_loss: 0.3953 - val_accuracy: 0.8504\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8458 - val_loss: 0.3797 - val_accuracy: 0.8610\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4136 - accuracy: 0.8510 - val_loss: 0.3695 - val_accuracy: 0.8572\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.4028 - accuracy: 0.8540 - val_loss: 0.3612 - val_accuracy: 0.8640\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3839 - accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model1.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model1.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('LeNet-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model in practice</td>\n",
       "      <td>0.9073</td>\n",
       "      <td>17.809999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>6.718439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  accuracy  training_time\n",
       "0  Model in practice    0.9073      17.809999\n",
       "1            LeNet-5    0.8611       6.718439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    result[r] = result[r][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5L-wVUuaoHU"
   },
   "source": [
    "# Exercise 14.2\n",
    "Construct VGG-like LeNet for MNIST\n",
    "- 3 conv. Layers: each layer has 2 convolutional 3x3 filters with ReLU activation\n",
    "  -> in - c1 - c2 - s3 - c4 - c5 - s6 - c7 - c8 - fc - out\n",
    "- Number of kernels: 6-16-120\n",
    "- padding: SAME\n",
    "- Max pooling with 2x2 mask and stride=2\n",
    "- FC: 84-10.\n",
    "- Dropout rate: 0.5\n",
    "- Output: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-ZUiK-wFaoHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 3)         111       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 3)         327       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 3)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 3)         2307      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 3)         2307      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 3)           129603    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 3)           129603    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                4116      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269224 (1.03 MB)\n",
      "Trainable params: 269224 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 14.2.1 Define VGG_like LeNet\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "    layers.Conv2D(filters=3, kernel_size=(6, 6), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(6, 6), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(120, 120), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(120, 120), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sEI90pAvaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 5s 22ms/step - loss: 1.5905 - accuracy: 0.3791 - val_loss: 0.8721 - val_accuracy: 0.6474\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.9097 - accuracy: 0.6422 - val_loss: 0.7146 - val_accuracy: 0.7194\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.7805 - accuracy: 0.7044 - val_loss: 0.6418 - val_accuracy: 0.7472\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.7085 - accuracy: 0.7342 - val_loss: 0.5999 - val_accuracy: 0.7662\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.6596 - accuracy: 0.7525 - val_loss: 0.5535 - val_accuracy: 0.7824\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.6119 - accuracy: 0.7686 - val_loss: 0.5614 - val_accuracy: 0.7746\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 2s 23ms/step - loss: 0.5878 - accuracy: 0.7768 - val_loss: 0.5218 - val_accuracy: 0.7928\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.5654 - accuracy: 0.7843 - val_loss: 0.5016 - val_accuracy: 0.8048\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.5516 - accuracy: 0.7905 - val_loss: 0.4970 - val_accuracy: 0.8066\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.5408 - accuracy: 0.7959 - val_loss: 0.4849 - val_accuracy: 0.8150\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5092 - accuracy: 0.8076\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model2.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model2.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('VGG like LeNet(kernel 6-16-120)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results by changing # of kernels, # neurons of FC1, # of conv. layers, batch normalization, and activation functions.\n",
    "\n",
    "- Compare results with LeNet: Accuracy, Training time\n",
    "\n",
    "2. Kernal수를 아래와 변경하여 학습후 결과를 비교하시오.\n",
    "Number of kernels: 16-32-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1wQqUaZvaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 3)         771       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 3)         2307      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 3)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 3)         9219      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 3)         9219      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 3)           36867     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 3)           36867     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                4116      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100216 (391.47 KB)\n",
      "Trainable params: 100216 (391.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 14.2.2 Define VGG_like LeNet with different number of kernels\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(32, 32), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(32, 32), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(64, 64), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(64, 64), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GLvXCySuaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 4s 18ms/step - loss: 1.6395 - accuracy: 0.3547 - val_loss: 0.9388 - val_accuracy: 0.6498\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 2s 17ms/step - loss: 0.9124 - accuracy: 0.6548 - val_loss: 0.7034 - val_accuracy: 0.7396\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 2s 18ms/step - loss: 0.7480 - accuracy: 0.7255 - val_loss: 0.6273 - val_accuracy: 0.7630\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 2s 18ms/step - loss: 0.6749 - accuracy: 0.7533 - val_loss: 0.5701 - val_accuracy: 0.7834\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 2s 18ms/step - loss: 0.6270 - accuracy: 0.7703 - val_loss: 0.5474 - val_accuracy: 0.7770\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 2s 17ms/step - loss: 0.5970 - accuracy: 0.7813 - val_loss: 0.5152 - val_accuracy: 0.8032\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.5708 - accuracy: 0.7887 - val_loss: 0.4914 - val_accuracy: 0.8046\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 2s 18ms/step - loss: 0.5512 - accuracy: 0.7950 - val_loss: 0.4745 - val_accuracy: 0.8052\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.5260 - accuracy: 0.8053 - val_loss: 0.4566 - val_accuracy: 0.8160\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 2s 18ms/step - loss: 0.5009 - accuracy: 0.8146 - val_loss: 0.4373 - val_accuracy: 0.8272\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4729 - accuracy: 0.8203\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model3.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model3.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('VGG like LeNet(kernel 16-32-64)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>6.718439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>26.488830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>22.708542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  accuracy  training_time\n",
       "0                          LeNet-5    0.8611       6.718439\n",
       "1  VGG like LeNet(kernel 6-16-120)    0.8076      26.488830\n",
       "2  VGG like LeNet(kernel 16-32-64)    0.8203      22.708542"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I6CaSZLaoHX"
   },
   "source": [
    "# Exercise 14.3  \n",
    "Resen-34를 이용하여 Fashion MNIST를 학습시키고자 한다.  그러나 ReseNet-34는 ImageNet data aize인 224x224에 맞게 구성되어 있으므로 이를 수정하여야 한다.\n",
    "1. ResNet-34에 28x28의 Fashion MNSIT data를 입력할 경우 featue size의 변화를 확인하시오.\n",
    "2. Fashion MNIST data의 경우 크기가 작으므로 첫번째 conv. layer에서 feature size를 줄이는 것은 적합하지 않다. ResNet-34를 수정하여 첫번째 conv. layer에서 feature size를 유지하도록 하고 학습시킨 결과를 확인하고 LeNet-5 및 VGG-like LeNet과 비교하시오.\n",
    "3. ImageNet을 위한 ResNet-34는 7x7 feature를 GlobalAveragePooling layer를 통과시켰다. Fashion MNIST에 대해서도 동일한 동작을 하도록 high layer를 제거하고 학습결과를 비교하시오. Kernel 수는 low layer로부터 시작한 값을 유지한다.  \n",
    "(Layer수가 줄었으므로 ResNet-34는 적합하지 않고 ResNet-16이 적합하나 편의상 ResNet-34로 부르기로 한다)\n",
    "4. 3번에서 kernel수가 64, 128일 때 residual layer를 각각 3, 4개씩 유지하였는데 이를 2, 3개로 줄이고 학습결과를 비교하시오.\n",
    "5. 2-4번의 결과를 보고 accuracy를 유지하는 범위내에서 네트워크 복잡도를 줄여 학습시간을 최소화하는 ResNet-34를 설계하고 학습결과를 비교하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.1.\n",
    "\n",
    "ResNet-34에 28x28의 Fashion MNSIT data를 입력할 경우 featue size의 변화를 확인하시오.\n",
    "    \n",
    "\n",
    "**feature size는 (28,28) -> (14,14) -> (7,7) -> (4,4) 로 변화한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UYBkzauaaoHX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        3136      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 14, 14, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 7, 7, 64)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 7, 7, 64)          74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 7, 7, 64)          74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 7, 7, 64)          74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 4, 4, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_7 (ResidualU  (None, 2, 2, 256)         920576    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_8 (ResidualU  (None, 2, 2, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_9 (ResidualU  (None, 2, 2, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_10 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_11 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_12 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_13 (Residual  (None, 1, 1, 512)         3676160   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_14 (Residual  (None, 1, 1, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_15 (Residual  (None, 1, 1, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21300554 (81.26 MB)\n",
      "Trainable params: 21283530 (81.19 MB)\n",
      "Non-trainable params: 17024 (66.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.1\n",
    "# Define ResNet-34 for 28x28 input\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4a = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4a.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4a.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model4a.add(tf.keras.layers.Flatten())\n",
    "model4a.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model4a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 14s 32ms/step - loss: 0.5857 - accuracy: 0.8055 - val_loss: 2.2488 - val_accuracy: 0.3944\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 3s 26ms/step - loss: 0.3180 - accuracy: 0.8830 - val_loss: 0.5155 - val_accuracy: 0.8220\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 3s 29ms/step - loss: 0.2623 - accuracy: 0.9027 - val_loss: 0.4864 - val_accuracy: 0.8494\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 3s 26ms/step - loss: 0.2259 - accuracy: 0.9148 - val_loss: 0.3684 - val_accuracy: 0.8714\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.2009 - accuracy: 0.9230 - val_loss: 0.4230 - val_accuracy: 0.8690\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 3s 27ms/step - loss: 0.1845 - accuracy: 0.9298 - val_loss: 0.4480 - val_accuracy: 0.8690\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1605 - accuracy: 0.9402 - val_loss: 0.5664 - val_accuracy: 0.8504\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 3s 27ms/step - loss: 0.1464 - accuracy: 0.9444 - val_loss: 0.5344 - val_accuracy: 0.8606\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.1280 - accuracy: 0.9507 - val_loss: 0.4688 - val_accuracy: 0.8774\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 3s 27ms/step - loss: 0.1158 - accuracy: 0.9570 - val_loss: 0.4384 - val_accuracy: 0.8788\n",
      "313/313 [==============================] - 3s 6ms/step - loss: 0.4854 - accuracy: 0.8713\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model4a.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4a.fit(X_train, y_train, epochs=n_epoch,\n",
    "                      validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model4a.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('14.3.1 ResNet-34')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.2.\n",
    "\n",
    "Fashion MNIST data의 경우 크기가 작으므로 첫번째 conv. layer에서 feature size를 줄이는 것은 적합하지 않다. ResNet-34를 수정하여 첫번째 conv. layer에서 feature size를 유지하도록 하고 학습시킨 결과를 확인하고 LeNet-5 및 VGG-like LeNet과 비교하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AQtaXbFaaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        64        \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 28, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 14, 14, 64)        74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 7, 7, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_7 (ResidualU  (None, 4, 4, 256)         920576    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_8 (ResidualU  (None, 4, 4, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_9 (ResidualU  (None, 4, 4, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_10 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_11 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_12 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_13 (Residual  (None, 2, 2, 512)         3676160   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_14 (Residual  (None, 2, 2, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_15 (Residual  (None, 2, 2, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21297482 (81.24 MB)\n",
      "Trainable params: 21280458 (81.18 MB)\n",
      "Non-trainable params: 17024 (66.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.2\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4b = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=1, strides=1, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4b.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4b.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model4b.add(tf.keras.layers.Flatten())\n",
    "model4b.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model4b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LYMNPhBLaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 16s 51ms/step - loss: 0.5662 - accuracy: 0.8057 - val_loss: 2.5077 - val_accuracy: 0.3788\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.3225 - accuracy: 0.8796 - val_loss: 0.5420 - val_accuracy: 0.8098\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.2722 - accuracy: 0.8977 - val_loss: 0.3653 - val_accuracy: 0.8670\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.2344 - accuracy: 0.9119 - val_loss: 0.4016 - val_accuracy: 0.8586\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.2078 - accuracy: 0.9225 - val_loss: 0.3676 - val_accuracy: 0.8778\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 5s 46ms/step - loss: 0.1821 - accuracy: 0.9308 - val_loss: 0.4003 - val_accuracy: 0.8692\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 5s 46ms/step - loss: 0.1570 - accuracy: 0.9404 - val_loss: 0.3782 - val_accuracy: 0.8800\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 5s 46ms/step - loss: 0.1340 - accuracy: 0.9490 - val_loss: 0.4191 - val_accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 5s 47ms/step - loss: 0.1205 - accuracy: 0.9549 - val_loss: 0.4130 - val_accuracy: 0.8782\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 5s 45ms/step - loss: 0.1066 - accuracy: 0.9595 - val_loss: 0.4389 - val_accuracy: 0.8800\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.4467 - accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model4b.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4b.fit(X_train, y_train, epochs=n_epoch,\n",
    "                      validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model4b.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('14.3.2 Revised ResNet-34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>6.718439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>26.488830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>22.708542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.3.1 ResNet-34</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>40.072834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.3.2 Revised ResNet-34</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>61.152374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  accuracy  training_time\n",
       "0                          LeNet-5    0.8611       6.718439\n",
       "1  VGG like LeNet(kernel 6-16-120)    0.8076      26.488830\n",
       "2  VGG like LeNet(kernel 16-32-64)    0.8203      22.708542\n",
       "3                 14.3.1 ResNet-34    0.8713      40.072834\n",
       "4         14.3.2 Revised ResNet-34    0.8804      61.152374"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.3.\n",
    "\n",
    "ImageNet을 위한 ResNet-34는 7x7 feature를 GlobalAveragePooling layer를 통과시켰다. Fashion MNIST에 대해서도 동일한 동작을 하도록 high layer를 제거하고 학습결과를 비교하시오. Kernel 수는 low layer로부터 시작한 값을 유지한다.  \n",
    "(Layer수가 줄었으므로 ResNet-34는 적합하지 않고 ResNet-16이 적합하나 편의상 ResNet-34로 부르기로 한다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "H3I1VcGJaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        64        \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 28, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 14, 14, 64)        74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 7, 7, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 128)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1343050 (5.12 MB)\n",
      "Trainable params: 1339850 (5.11 MB)\n",
      "Non-trainable params: 3200 (12.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.3\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4c = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=1, strides=1, input_shape=[28, 28, 1]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4c.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4c.add(layers.GlobalAveragePooling2D())\n",
    "model4c.add(layers.Flatten())\n",
    "model4c.add(layers.Dense(10, activation=\"softmax\"))\n",
    "model4c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-EXVCatbaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 7s 29ms/step - loss: 0.5485 - accuracy: 0.8008 - val_loss: 2.0652 - val_accuracy: 0.4056\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.3331 - accuracy: 0.8767 - val_loss: 0.7727 - val_accuracy: 0.7274\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.2833 - accuracy: 0.8947 - val_loss: 0.4182 - val_accuracy: 0.8468\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.2552 - accuracy: 0.9056 - val_loss: 0.4735 - val_accuracy: 0.8344\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.2254 - accuracy: 0.9162 - val_loss: 0.4979 - val_accuracy: 0.8406\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.2069 - accuracy: 0.9224 - val_loss: 0.5728 - val_accuracy: 0.8180\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.1775 - accuracy: 0.9345 - val_loss: 0.3996 - val_accuracy: 0.8626\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.1573 - accuracy: 0.9418 - val_loss: 0.5389 - val_accuracy: 0.8452\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.1339 - accuracy: 0.9502 - val_loss: 0.4894 - val_accuracy: 0.8540\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.1135 - accuracy: 0.9575 - val_loss: 0.5984 - val_accuracy: 0.8522\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5944 - accuracy: 0.8544\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model4c.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4c.fit(X_train, y_train, epochs=n_epoch,\n",
    "                      validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model4c.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('14.3.3 Revised ResNet-34 with 7x7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>6.718439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>26.488830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>22.708542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.3.1 ResNet-34</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>40.072834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.3.2 Revised ResNet-34</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>61.152374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.3.3 Revised ResNet-34 with 7x7</td>\n",
       "      <td>0.8544</td>\n",
       "      <td>32.327105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  accuracy  training_time\n",
       "0                            LeNet-5    0.8611       6.718439\n",
       "1    VGG like LeNet(kernel 6-16-120)    0.8076      26.488830\n",
       "2    VGG like LeNet(kernel 16-32-64)    0.8203      22.708542\n",
       "3                   14.3.1 ResNet-34    0.8713      40.072834\n",
       "4           14.3.2 Revised ResNet-34    0.8804      61.152374\n",
       "5  14.3.3 Revised ResNet-34 with 7x7    0.8544      32.327105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.4.\n",
    "\n",
    "4. 3번에서 kernel수가 64, 128일 때 residual layer를 각각 3, 4개씩 유지하였는데 이를 2, 3개로 줄이고 학습결과를 비교하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5ZPMOfm1aoHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        64        \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 28, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 14, 14, 64)        74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 7, 7, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 128)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972874 (3.71 MB)\n",
      "Trainable params: 970442 (3.70 MB)\n",
      "Non-trainable params: 2432 (9.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.4\n",
    "# Modify ResNet-34\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4d = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=1, strides=1, input_shape=[28, 28, 1]),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "\n",
    "prev_filters = 64\n",
    "for filters in [64] * 2 + [128] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4d.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4d.add(layers.GlobalAveragePooling2D())\n",
    "model4d.add(layers.Flatten())\n",
    "model4d.add(layers.Dense(10, activation=\"softmax\"))\n",
    "model4d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1eZr5DppaoHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 6s 21ms/step - loss: 0.5535 - accuracy: 0.8010 - val_loss: 3.3451 - val_accuracy: 0.2774\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.3381 - accuracy: 0.8760 - val_loss: 1.9468 - val_accuracy: 0.3568\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.2896 - accuracy: 0.8934 - val_loss: 0.5413 - val_accuracy: 0.7980\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.2633 - accuracy: 0.9017 - val_loss: 0.4931 - val_accuracy: 0.8258\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.2366 - accuracy: 0.9113 - val_loss: 0.4411 - val_accuracy: 0.8592\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.2140 - accuracy: 0.9199 - val_loss: 0.6112 - val_accuracy: 0.8144\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.1887 - accuracy: 0.9308 - val_loss: 0.4704 - val_accuracy: 0.8490\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.1701 - accuracy: 0.9363 - val_loss: 0.3743 - val_accuracy: 0.8754\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.1439 - accuracy: 0.9474 - val_loss: 0.6576 - val_accuracy: 0.8280\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 2s 19ms/step - loss: 0.1278 - accuracy: 0.9534 - val_loss: 1.0738 - val_accuracy: 0.7778\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.1104 - accuracy: 0.7635\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model4d.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4d.fit(X_train, y_train, epochs=n_epoch,\n",
    "                      validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model4d.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('14.3.4 Revised ResNet-34 reduced layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8611</td>\n",
       "      <td>6.718439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>26.488830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8203</td>\n",
       "      <td>22.708542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.3.1 ResNet-34</td>\n",
       "      <td>0.8713</td>\n",
       "      <td>40.072834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.3.2 Revised ResNet-34</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>61.152374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.3.3 Revised ResNet-34 with 7x7</td>\n",
       "      <td>0.8544</td>\n",
       "      <td>32.327105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.3.4 Revised ResNet-34 reduced layers</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>25.274931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  accuracy  training_time\n",
       "0                                  LeNet-5    0.8611       6.718439\n",
       "1          VGG like LeNet(kernel 6-16-120)    0.8076      26.488830\n",
       "2          VGG like LeNet(kernel 16-32-64)    0.8203      22.708542\n",
       "3                         14.3.1 ResNet-34    0.8713      40.072834\n",
       "4                 14.3.2 Revised ResNet-34    0.8804      61.152374\n",
       "5        14.3.3 Revised ResNet-34 with 7x7    0.8544      32.327105\n",
       "6  14.3.4 Revised ResNet-34 reduced layers    0.7635      25.274931"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.5.\n",
    "\n",
    "5. 2-4번의 결과를 보고 accuracy를 유지하는 범위내에서 네트워크 복잡도를 줄여 학습시간을 최소화하는 ResNet-34를 설계하고 학습결과를 비교하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7_VVF9hBaoHZ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model4e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel4e\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model4e' is not defined"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.5\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model4e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l02WUzLAaoHZ"
   },
   "outputs": [],
   "source": [
    "# Compile, train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB5EIFRsaoHa"
   },
   "source": [
    "# Exercise 14.4   \n",
    "1. 위의 셀들을 참조하여 base_model을 MobileNet으로 변경하여 학습시키시오.  \n",
    "2. Xception과 학습시간 및 정확도를 비교하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqxnSBJ3pKz8"
   },
   "source": [
    "Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # extra code – resets layer name counter\n",
    "\n",
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 18))\n",
    "\n",
    "index = 0\n",
    "for image, label in valid_set_raw.take(5):\n",
    "    index += 1\n",
    "    plt.subplot(9, 2, 2 * index - 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Before\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # 전처리된 이미지\n",
    "    processed_image = preprocess(tf.expand_dims(image, 0))\n",
    "    plt.subplot(9, 2, 2 * index)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(processed_image[0]))\n",
    "    plt.title(f\"After\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'Model': [], 'accuracy': [], 'training_time': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model6 = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "histroy = model6.fit(train_set, validation_data=valid_set, epochs=3, batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model6.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model6.fit(train_set, validation_data=valid_set, epochs=10, batch_size=BATCH_SIZE)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model6.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Xception w/ layer(56~) trainable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE8YQucvaoHa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 14.4.1\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.MobileNet(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model7 = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model7.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=3)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in zip(range(43), range(43, 86)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsyU2wLWaoHv"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers[73:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet w/ layer(73~) trainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX3HHaUlaoHv"
   },
   "outputs": [],
   "source": [
    "# Compile and train all layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet w/ layer(all) trainable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIrP3W2GaoHw"
   },
   "source": [
    "#### 14.4.2  비교결과  \n",
    "1.  Xception 결과\n",
    "2.  Mobilenet 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl9zizRopK0C"
   },
   "source": [
    "# Exercise 14.5\n",
    "_Exercise: Go through TensorFlow's [Style Transfer tutorial](https://homl.info/styletuto). It is a fun way to generate art using Deep Learning._  \n",
    "위의 tutorial 코드를 노트북에서 실행하여 결과를 제출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import IPython.display as display\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = tf.keras.utils.get_file(\n",
    "    'YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "style_path = tf.keras.utils.get_file(\n",
    "    'kandinsky5.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
    "tensor_to_image(stylized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2']\n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on ImageNet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "# Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    print(name)\n",
    "    print(\"  shape: \", output.numpy().shape)\n",
    "    print(\"  min: \", output.numpy().min())\n",
    "    print(\"  max: \", output.numpy().max())\n",
    "    print(\"  mean: \", output.numpy().mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg = vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                          outputs[self.num_style_layers:])\n",
    "\n",
    "        style_outputs = [gram_matrix(style_output)\n",
    "                         for style_output in style_outputs]\n",
    "\n",
    "        content_dict = {content_name: value\n",
    "                        for content_name, value\n",
    "                        in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        style_dict = {style_name: value\n",
    "                      for style_name, value\n",
    "                      in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "        return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight = 1e-2\n",
    "content_weight = 1e4\n",
    "\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))\n",
    "\n",
    "\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "        print(\".\", end='', flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))\n",
    "    print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_pass_x_y(image):\n",
    "    x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
    "    y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
    "\n",
    "    return x_var, y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
    "\n",
    "x_deltas, y_deltas = high_pass_x_y(image)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(clip_0_1(sobel[..., 0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(clip_0_1(sobel[..., 1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    x_deltas, y_deltas = high_pass_x_y(image)\n",
    "    return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
    "\n",
    "\n",
    "total_variation_loss(image).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.image.total_variation(image).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight*tf.image.total_variation(image)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "        print(\".\", end='', flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))\n",
    "    print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'stylized-image.png'\n",
    "tensor_to_image(image).save(file_name)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
