{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK7ecnb6pKzp"
   },
   "source": [
    "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6cR-I1WpKzs"
   },
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 14._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPbJEmZpKzu"
   },
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version:  1.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:18:26.017023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-17 01:18:26.017060: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-17 01:18:26.017093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-17 01:18:26.023652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 01:18:26.609056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.14.0\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:18:27.589444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:27.617326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:27.617662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:27.622227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:27.622521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:27.622794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:28.126082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:28.126385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:28.126655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-17 01:18:28.126877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46681 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:c2:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)\n",
    "from packaging import version\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "import tensorflow as tf\n",
    "print(\"TF version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oid44Xx-pKz6"
   },
   "source": [
    "# CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZe7PLfpKz6"
   },
   "source": [
    "**Tackling Fashion MNIST With a CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads the mnist dataset, add the channels axis to the inputs,\n",
    "#              scales the values to the 0-1 range, and splits the dataset\n",
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'Model':[],'accuracy': [], 'training_time': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL6mFjsUaoHT"
   },
   "source": [
    "# Exercise 14.1\n",
    "- Construct simplified LeNet-5 as shown in the table\n",
    "- ReLu activation\n",
    "- Ignore S2->C3 connection and consider regular connection\n",
    "- Dropout rate: 0.5 for FC\n",
    "- Output layer: softmax\n",
    "- Train and evalute the LeNet-5 model and compare the results of the model in the practice code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QK1AM86ZaoHT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 120)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                10164     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define simplified LeNet-5\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=120, kernel_size=(5, 5), activation='tanh'),\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='tanh'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_x17eKUjaoHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 01:18:30.604445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8800\n",
      "2024-05-17 01:18:32.014818: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7054ada93e50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-17 01:18:32.014844: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-05-17 01:18:32.018468: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-17 01:18:32.099458: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 6s 4ms/step - loss: 0.8049 - accuracy: 0.7039 - val_loss: 0.5169 - val_accuracy: 0.8008\n",
      "Epoch 2/10\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 0.5482 - accuracy: 0.8036 - val_loss: 0.4725 - val_accuracy: 0.8280\n",
      "Epoch 3/10\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 0.4800 - accuracy: 0.8288 - val_loss: 0.4205 - val_accuracy: 0.8458\n",
      "Epoch 4/10\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 0.4361 - accuracy: 0.8467 - val_loss: 0.3922 - val_accuracy: 0.8568\n",
      "Epoch 5/10\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 0.4098 - accuracy: 0.8546 - val_loss: 0.3822 - val_accuracy: 0.8568\n",
      "Epoch 6/10\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 0.3847 - accuracy: 0.8642 - val_loss: 0.3478 - val_accuracy: 0.8698\n",
      "Epoch 7/10\n",
      "275/275 [==============================] - 1s 4ms/step - loss: 0.3663 - accuracy: 0.8708 - val_loss: 0.3398 - val_accuracy: 0.8756\n",
      "Epoch 8/10\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 0.3540 - accuracy: 0.8753 - val_loss: 0.3330 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "275/275 [==============================] - 1s 5ms/step - loss: 0.3393 - accuracy: 0.8787 - val_loss: 0.3310 - val_accuracy: 0.8762\n",
      "Epoch 10/10\n",
      "275/275 [==============================] - 1s 3ms/step - loss: 0.3288 - accuracy: 0.8819 - val_loss: 0.3175 - val_accuracy: 0.8770\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model1.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model1.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('LeNet-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5L-wVUuaoHU"
   },
   "source": [
    "# Exercise 14.2\n",
    "Construct VGG-like LeNet for MNIST\n",
    "- 3 conv. Layers: each layer has 2 convolutional 3x3 filters with ReLU activation\n",
    "  -> in - c1 - c2 - s3 - c4 - c5 - s6 - c7 - c8 - fc - out\n",
    "- Number of kernels: 6-16-120\n",
    "- padding: SAME\n",
    "- Max pooling with 2x2 mask and stride=2\n",
    "- FC: 84-10.\n",
    "- Dropout rate: 0.5\n",
    "- Output: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-ZUiK-wFaoHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 3)         111       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 3)         327       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 3)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 3)         2307      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 3)         2307      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 3)           129603    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 3)           129603    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                4116      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269224 (1.03 MB)\n",
      "Trainable params: 269224 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 14.2.1 Define VGG_like LeNet\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "    layers.Conv2D(filters=3, kernel_size=(6, 6), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(6, 6), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(120, 120), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(120, 120), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sEI90pAvaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "275/275 [==============================] - 5s 12ms/step - loss: 1.2817 - accuracy: 0.5019 - val_loss: 0.6947 - val_accuracy: 0.7342\n",
      "Epoch 2/10\n",
      "275/275 [==============================] - 3s 11ms/step - loss: 0.7283 - accuracy: 0.7319 - val_loss: 0.5662 - val_accuracy: 0.7728\n",
      "Epoch 3/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.6313 - accuracy: 0.7690 - val_loss: 0.5204 - val_accuracy: 0.8016\n",
      "Epoch 4/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.5767 - accuracy: 0.7909 - val_loss: 0.4714 - val_accuracy: 0.8190\n",
      "Epoch 5/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.5376 - accuracy: 0.8053 - val_loss: 0.4492 - val_accuracy: 0.8272\n",
      "Epoch 6/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.5133 - accuracy: 0.8135 - val_loss: 0.4409 - val_accuracy: 0.8352\n",
      "Epoch 7/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.4871 - accuracy: 0.8224 - val_loss: 0.4160 - val_accuracy: 0.8410\n",
      "Epoch 8/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.4667 - accuracy: 0.8298 - val_loss: 0.4158 - val_accuracy: 0.8356\n",
      "Epoch 9/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.4530 - accuracy: 0.8346 - val_loss: 0.4017 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "275/275 [==============================] - 3s 12ms/step - loss: 0.4337 - accuracy: 0.8414 - val_loss: 0.3924 - val_accuracy: 0.8448\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.4220 - accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model2.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model2.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('VGG like LeNet(kernel 6-16-120)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results by changing # of kernels, # neurons of FC1, # of conv. layers, batch normalization, and activation functions.\n",
    "\n",
    "- Compare results with LeNet: Accuracy, Training time\n",
    "\n",
    "2. Kernal수를 아래와 변경하여 학습후 결과를 비교하시오.\n",
    "Number of kernels: 16-32-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1wQqUaZvaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " zero_padding2d (ZeroPaddin  (None, 32, 32, 1)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 3)         771       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 3)         2307      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 3)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 3)         9219      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 3)         9219      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 3)           36867     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 3)           36867     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 3)           0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                4116      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100216 (391.47 KB)\n",
      "Trainable params: 100216 (391.47 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 14.2.2 Define VGG_like LeNet with different number of kernels\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "    layers.Input(shape=(28, 28, 1)),\n",
    "    layers.ZeroPadding2D(padding=(2, 2)),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(16, 16), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(32, 32), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(32, 32), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Conv2D(filters=3, kernel_size=(64, 64), activation='relu', input_shape=(32, 32, 1), padding='SAME',),\n",
    "    layers.Conv2D(filters=3, kernel_size=(64, 64), activation='relu', padding='SAME',),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GLvXCySuaoHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "275/275 [==============================] - 4s 9ms/step - loss: 1.1555 - accuracy: 0.5703 - val_loss: 0.6719 - val_accuracy: 0.7484\n",
      "Epoch 2/10\n",
      "275/275 [==============================] - 2s 9ms/step - loss: 0.7177 - accuracy: 0.7423 - val_loss: 0.5620 - val_accuracy: 0.7902\n",
      "Epoch 3/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.6022 - accuracy: 0.7802 - val_loss: 0.4849 - val_accuracy: 0.8158\n",
      "Epoch 4/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.5471 - accuracy: 0.8028 - val_loss: 0.4543 - val_accuracy: 0.8304\n",
      "Epoch 5/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.4984 - accuracy: 0.8198 - val_loss: 0.4300 - val_accuracy: 0.8410\n",
      "Epoch 6/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.4669 - accuracy: 0.8311 - val_loss: 0.4287 - val_accuracy: 0.8340\n",
      "Epoch 7/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.4335 - accuracy: 0.8438 - val_loss: 0.3935 - val_accuracy: 0.8506\n",
      "Epoch 8/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.4173 - accuracy: 0.8506 - val_loss: 0.3876 - val_accuracy: 0.8556\n",
      "Epoch 9/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.4064 - accuracy: 0.8555 - val_loss: 0.3822 - val_accuracy: 0.8558\n",
      "Epoch 10/10\n",
      "275/275 [==============================] - 3s 9ms/step - loss: 0.3858 - accuracy: 0.8621 - val_loss: 0.3724 - val_accuracy: 0.8594\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8549\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model3.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model3.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('VGG like LeNet(kernel 16-32-64)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>15.139677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>35.560174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8549</td>\n",
       "      <td>27.708264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  accuracy  training_time\n",
       "0                          LeNet-5    0.8762      15.139677\n",
       "1  VGG like LeNet(kernel 6-16-120)    0.8428      35.560174\n",
       "2  VGG like LeNet(kernel 16-32-64)    0.8549      27.708264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I6CaSZLaoHX"
   },
   "source": [
    "# Exercise 14.3  \n",
    "Resen-34를 이용하여 Fashion MNIST를 학습시키고자 한다.  그러나 ReseNet-34는 ImageNet data aize인 224x224에 맞게 구성되어 있으므로 이를 수정하여야 한다.\n",
    "1. ResNet-34에 28x28의 Fashion MNSIT data를 입력할 경우 featue size의 변화를 확인하시오.\n",
    "2. Fashion MNIST data의 경우 크기가 작으므로 첫번째 conv. layer에서 feature size를 줄이는 것은 적합하지 않다. ResNet-34를 수정하여 첫번째 conv. layer에서 feature size를 유지하도록 하고 학습시킨 결과를 확인하고 LeNet-5 및 VGG-like LeNet과 비교하시오.\n",
    "3. ImageNet을 위한 ResNet-34는 7x7 feature를 GlobalAveragePooling layer를 통과시켰다. Fashion MNIST에 대해서도 동일한 동작을 하도록 high layer를 제거하고 학습결과를 비교하시오. Kernel 수는 low layer로부터 시작한 값을 유지한다.  \n",
    "(Layer수가 줄었으므로 ResNet-34는 적합하지 않고 ResNet-16이 적합하나 편의상 ResNet-34로 부르기로 한다)\n",
    "4. 3번에서 kernel수가 64, 128일 때 residual layer를 각각 3, 4개씩 유지하였는데 이를 2, 3개로 줄이고 학습결과를 비교하시오.\n",
    "5. 2-4번의 결과를 보고 accuracy를 유지하는 범위내에서 네트워크 복잡도를 줄여 학습시간을 최소화하는 ResNet-34를 설계하고 학습결과를 비교하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.1.\n",
    "\n",
    "ResNet-34에 28x28의 Fashion MNSIT data를 입력할 경우 featue size의 변화를 확인하시오.\n",
    "    \n",
    "\n",
    "**feature size는 (28,28) -> (14,14) -> (7,7) -> (4,4) 로 변화한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UYBkzauaaoHX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        9408      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 14, 14, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 7, 7, 64)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 7, 7, 64)          74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 7, 7, 64)          74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 7, 7, 64)          74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 4, 4, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualU  (None, 4, 4, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_7 (ResidualU  (None, 2, 2, 256)         920576    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_8 (ResidualU  (None, 2, 2, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_9 (ResidualU  (None, 2, 2, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_10 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_11 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_12 (Residual  (None, 2, 2, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_13 (Residual  (None, 1, 1, 512)         3676160   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_14 (Residual  (None, 1, 1, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_15 (Residual  (None, 1, 1, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21306826 (81.28 MB)\n",
      "Trainable params: 21289802 (81.21 MB)\n",
      "Non-trainable params: 17024 (66.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.1\n",
    "# Define ResNet-34 for 28x28 input\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4a = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[28, 28, 3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4a.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4a.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model4a.add(tf.keras.layers.Flatten())\n",
    "model4a.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model4a.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.2.\n",
    "\n",
    "Fashion MNIST data의 경우 크기가 작으므로 첫번째 conv. layer에서 feature size를 줄이는 것은 적합하지 않다. ResNet-34를 수정하여 첫번째 conv. layer에서 feature size를 유지하도록 하고 학습시킨 결과를 확인하고 LeNet-5 및 VGG-like LeNet과 비교하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AQtaXbFaaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        64        \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 28, 28, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " residual_unit (ResidualUni  (None, 14, 14, 64)        74240     \n",
      " t)                                                              \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualU  (None, 14, 14, 64)        74240     \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualU  (None, 7, 7, 128)         230912    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualU  (None, 7, 7, 128)         295936    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_7 (ResidualU  (None, 4, 4, 256)         920576    \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_8 (ResidualU  (None, 4, 4, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_9 (ResidualU  (None, 4, 4, 256)         1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_10 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_11 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_12 (Residual  (None, 4, 4, 256)         1181696   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_13 (Residual  (None, 2, 2, 512)         3676160   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_14 (Residual  (None, 2, 2, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " residual_unit_15 (Residual  (None, 2, 2, 512)         4722688   \n",
      " Unit)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21297482 (81.24 MB)\n",
      "Trainable params: 21280458 (81.18 MB)\n",
      "Non-trainable params: 17024 (66.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ex. 14.3.2\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4b = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=1, strides=1, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation(\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "])\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model4b.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "model4b.add(tf.keras.layers.GlobalAvgPool2D())\n",
    "model4b.add(tf.keras.layers.Flatten())\n",
    "model4b.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "model4b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LYMNPhBLaoHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "275/275 [==============================] - 18s 27ms/step - loss: 0.5006 - accuracy: 0.8222 - val_loss: 0.7659 - val_accuracy: 0.7436\n",
      "Epoch 2/10\n",
      "275/275 [==============================] - 8s 27ms/step - loss: 0.3316 - accuracy: 0.8770 - val_loss: 0.3716 - val_accuracy: 0.8640\n",
      "Epoch 3/10\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.2823 - accuracy: 0.8944 - val_loss: 0.3551 - val_accuracy: 0.8736\n",
      "Epoch 4/10\n",
      "275/275 [==============================] - 8s 28ms/step - loss: 0.2438 - accuracy: 0.9091 - val_loss: 0.3526 - val_accuracy: 0.8774\n",
      "Epoch 5/10\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.2178 - accuracy: 0.9178 - val_loss: 0.3236 - val_accuracy: 0.8852\n",
      "Epoch 6/10\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.1886 - accuracy: 0.9295 - val_loss: 0.3716 - val_accuracy: 0.8788\n",
      "Epoch 7/10\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.1721 - accuracy: 0.9353 - val_loss: 0.3263 - val_accuracy: 0.8872\n",
      "Epoch 8/10\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.1457 - accuracy: 0.9451 - val_loss: 0.3156 - val_accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "275/275 [==============================] - 7s 26ms/step - loss: 0.1298 - accuracy: 0.9510 - val_loss: 0.3367 - val_accuracy: 0.8952\n",
      "Epoch 10/10\n",
      "275/275 [==============================] - 7s 27ms/step - loss: 0.1089 - accuracy: 0.9594 - val_loss: 0.4542 - val_accuracy: 0.8712\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.4653 - accuracy: 0.8767\n"
     ]
    }
   ],
   "source": [
    "# Compile, train and evaluate\n",
    "n_epoch = 10\n",
    "model4b.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4b.fit(X_train, y_train, epochs=n_epoch,\n",
    "                     validation_data=(X_valid, y_valid), batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model4b.evaluate(X_test, y_test)\n",
    "\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Revised ResNet-34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeNet-5</td>\n",
       "      <td>0.8762</td>\n",
       "      <td>15.139677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG like LeNet(kernel 6-16-120)</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>35.560174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG like LeNet(kernel 16-32-64)</td>\n",
       "      <td>0.8549</td>\n",
       "      <td>27.708264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Revised ResNet-34</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>84.369976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  accuracy  training_time\n",
       "0                          LeNet-5    0.8762      15.139677\n",
       "1  VGG like LeNet(kernel 6-16-120)    0.8428      35.560174\n",
       "2  VGG like LeNet(kernel 16-32-64)    0.8549      27.708264\n",
       "3                Revised ResNet-34    0.8767      84.369976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3.3.\n",
    "\n",
    "ImageNet을 위한 ResNet-34는 7x7 feature를 GlobalAveragePooling layer를 통과시켰다. Fashion MNIST에 대해서도 동일한 동작을 하도록 high layer를 제거하고 학습결과를 비교하시오. Kernel 수는 low layer로부터 시작한 값을 유지한다.  \n",
    "(Layer수가 줄었으므로 ResNet-34는 적합하지 않고 ResNet-16이 적합하나 편의상 ResNet-34로 부르기로 한다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model4a.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: conv2d                \n",
      "  1: batch_normalization   \n",
      "  2: activation            \n",
      "  3: max_pooling2d         \n",
      "  4: residual_unit         \n",
      "  5: residual_unit_1       \n",
      "  6: residual_unit_2       \n",
      "  7: residual_unit_3       \n",
      "  8: residual_unit_4       \n",
      "  9: residual_unit_5       \n",
      " 10: residual_unit_6       \n",
      " 11: residual_unit_7       \n",
      " 12: residual_unit_8       \n",
      " 13: residual_unit_9       \n",
      " 14: residual_unit_10      \n",
      " 15: residual_unit_11      \n",
      " 16: residual_unit_12      \n",
      " 17: residual_unit_13      \n",
      " 18: residual_unit_14      \n",
      " 19: residual_unit_15      \n",
      " 20: global_average_pooling2d\n",
      " 21: flatten               \n",
      " 22: dense                 \n"
     ]
    }
   ],
   "source": [
    "for indices in zip(range(23)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {model4a.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3I1VcGJaoHY"
   },
   "outputs": [],
   "source": [
    "# Ex. 14.3.3\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model4c ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EXVCatbaoHY"
   },
   "outputs": [],
   "source": [
    "# Compile, train and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZPMOfm1aoHZ"
   },
   "outputs": [],
   "source": [
    "# Ex. 14.3.4\n",
    "# Modify ResNet-34\n",
    "\n",
    "model4d ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eZr5DppaoHZ"
   },
   "outputs": [],
   "source": [
    "# Compile, train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_VVF9hBaoHZ"
   },
   "outputs": [],
   "source": [
    "# Ex. 14.3.5\n",
    "# Modify ResNet-34\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model4e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l02WUzLAaoHZ"
   },
   "outputs": [],
   "source": [
    "# Compile, train and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB5EIFRsaoHa"
   },
   "source": [
    "# Exercise 14.4   \n",
    "1. 위의 셀들을 참조하여 base_model을 MobileNet으로 변경하여 학습시키시오.  \n",
    "2. Xception과 학습시간 및 정확도를 비교하시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqxnSBJ3pKz8"
   },
   "source": [
    "Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # extra code – resets layer name counter\n",
    "\n",
    "batch_size = 32\n",
    "preprocess = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n",
    "])\n",
    "train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n",
    "train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n",
    "test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 18))\n",
    "\n",
    "index = 0\n",
    "for image, label in valid_set_raw.take(5):\n",
    "    index += 1\n",
    "    plt.subplot(9, 2, 2 * index - 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Before\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # 전처리된 이미지\n",
    "    processed_image = preprocess(tf.expand_dims(image, 0))\n",
    "    plt.subplot(9, 2, 2 * index)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(processed_image[0]))\n",
    "    plt.title(f\"After\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'Model':[],'accuracy': [], 'training_time': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
    "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
    "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                     include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model6 = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "histroy = model6.fit(train_set, validation_data=valid_set, epochs=3, batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model6.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[56:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model6.fit(train_set, validation_data=valid_set, epochs=10, batch_size=200)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model6.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Xception w/ layer(56~) trainable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE8YQucvaoHa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#14.4.1\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility\n",
    "base_model = tf.keras.applications.MobileNet(weights=\"imagenet\", include_top=False)\n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model7 = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "model7.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=3)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in zip(range(43), range(43, 86)):\n",
    "    for idx in indices:\n",
    "        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsyU2wLWaoHv"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers[73:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet w/ layer(73~) trainable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX3HHaUlaoHv"
   },
   "outputs": [],
   "source": [
    "# Compile and train all layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model6.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "start_time = time.time()\n",
    "histroy = model7.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "end_time = time.time()\n",
    "\n",
    "score = model7.evaluate(valid_set)\n",
    "result['accuracy'].append(score[1])\n",
    "result['training_time'].append(end_time-start_time)\n",
    "result['Model'].append('Mobilenet w/ layer(all) trainable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIrP3W2GaoHw"
   },
   "source": [
    "#### 14.4.2  비교결과  \n",
    "1.  Xception 결과\n",
    "2.  Mobilenet 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl9zizRopK0C"
   },
   "source": [
    "# Exercise 14.5\n",
    "_Exercise: Go through TensorFlow's [Style Transfer tutorial](https://homl.info/styletuto). It is a fun way to generate art using Deep Learning._  \n",
    "위의 tutorial 코드를 노트북에서 실행하여 결과를 제출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import IPython.display as display\n",
    "mpl.rcParams['figure.figsize'] = (12, 12)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor) > 3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = tf.keras.utils.get_file(\n",
    "    'YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
    "style_path = tf.keras.utils.get_file(\n",
    "    'kandinsky5.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    img = img[tf.newaxis, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
    "tensor_to_image(stylized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2']\n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on ImageNet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "# Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    print(name)\n",
    "    print(\"  shape: \", output.numpy().shape)\n",
    "    print(\"  min: \", output.numpy().min())\n",
    "    print(\"  max: \", output.numpy().max())\n",
    "    print(\"  mean: \", output.numpy().mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg = vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                          outputs[self.num_style_layers:])\n",
    "\n",
    "        style_outputs = [gram_matrix(style_output)\n",
    "                         for style_output in style_outputs]\n",
    "\n",
    "        content_dict = {content_name: value\n",
    "                        for content_name, value\n",
    "                        in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        style_dict = {style_name: value\n",
    "                      for style_name, value\n",
    "                      in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "        return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight = 1e-2\n",
    "content_weight = 1e4\n",
    "\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))\n",
    "\n",
    "\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "tensor_to_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "        print(\".\", end='', flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(image))\n",
    "    print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_pass_x_y(image):\n",
    "  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
    "  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
    "\n",
    "  return x_var, y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
    "\n",
    "x_deltas, y_deltas = high_pass_x_y(image)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(clip_0_1(sobel[..., 0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(clip_0_1(sobel[..., 1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "  x_deltas, y_deltas = high_pass_x_y(image)\n",
    "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
    "total_variation_loss(image).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.image.total_variation(image).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight*tf.image.total_variation(image)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "  for m in range(steps_per_epoch):\n",
    "    step += 1\n",
    "    train_step(image)\n",
    "    print(\".\", end='', flush=True)\n",
    "  display.clear_output(wait=True)\n",
    "  display.display(tensor_to_image(image))\n",
    "  print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'stylized-image.png'\n",
    "tensor_to_image(image).save(file_name)\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "  files.download(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
